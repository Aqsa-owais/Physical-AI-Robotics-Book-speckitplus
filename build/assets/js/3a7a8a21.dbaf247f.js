"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[845],{1184:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"module-2-digital-twin/sensor-simulation","title":"Sensor Simulation: Cameras, LiDAR, IMUs","description":"Introduction","source":"@site/docs/module-2-digital-twin/sensor-simulation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/sensor-simulation","permalink":"/giaic-hackathon-speckit-plus/docs/module-2-digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-digital-twin/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Physics Simulation Fundamentals: Gravity, Collisions, and Constraints","permalink":"/giaic-hackathon-speckit-plus/docs/module-2-digital-twin/physics-simulation"},"next":{"title":"Digital Twin Environments: Gazebo Usage and Best Practices","permalink":"/giaic-hackathon-speckit-plus/docs/module-2-digital-twin/environments"}}');var s=a(4848),r=a(8453);const t={sidebar_position:5},o="Sensor Simulation: Cameras, LiDAR, IMUs",l={},m=[{value:"Introduction",id:"introduction",level:2},{value:"Camera Simulation",id:"camera-simulation",level:2},{value:"Basic Camera Configuration",id:"basic-camera-configuration",level:3},{value:"Advanced Camera Properties",id:"advanced-camera-properties",level:3},{value:"Stereo Camera Configuration",id:"stereo-camera-configuration",level:3},{value:"Camera Integration with ROS 2",id:"camera-integration-with-ros-2",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"2D LiDAR Configuration",id:"2d-lidar-configuration",level:3},{value:"3D LiDAR Configuration (Velodyne-style)",id:"3d-lidar-configuration-velodyne-style",level:3},{value:"LiDAR Integration with ROS 2",id:"lidar-integration-with-ros-2",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"Basic IMU Configuration",id:"basic-imu-configuration",level:3},{value:"Advanced IMU with Bias and Drift",id:"advanced-imu-with-bias-and-drift",level:3},{value:"IMU Integration with ROS 2",id:"imu-integration-with-ros-2",level:3},{value:"Multi-Sensor Fusion Simulation",id:"multi-sensor-fusion-simulation",level:2},{value:"Sensor Accuracy and Calibration",id:"sensor-accuracy-and-calibration",level:2},{value:"Modeling Sensor Imperfections",id:"modeling-sensor-imperfections",level:3},{value:"Sensor Calibration in Simulation",id:"sensor-calibration-in-simulation",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Optimizing Sensor Simulation",id:"optimizing-sensor-simulation",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Summary",id:"summary",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"sensor-simulation-cameras-lidar-imus",children:"Sensor Simulation: Cameras, LiDAR, IMUs"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(e.p,{children:"Sensor simulation is a critical component of Digital Twin systems, allowing robots to perceive their virtual environment in ways that closely match real-world sensor capabilities. Properly simulated sensors enable robots to develop perception algorithms, test navigation systems, and validate control strategies in a safe, controlled environment before deployment on physical hardware."}),"\n",(0,s.jsx)(e.p,{children:"This guide covers the simulation of three fundamental sensor types used in robotics: cameras for vision, LiDAR for 3D mapping and obstacle detection, and IMUs for orientation and acceleration measurement."}),"\n",(0,s.jsx)(e.h2,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Cameras are essential for visual perception in robotics. Simulated cameras must reproduce the characteristics of real cameras including field of view, resolution, distortion, and noise patterns."}),"\n",(0,s.jsx)(e.h3,{id:"basic-camera-configuration",children:"Basic Camera Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="camera" type="camera">\n  <always_on>1</always_on>\n  <visualize>true</visualize>\n  <update_rate>30.0</update_rate>\n  <camera name="head">\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees in radians --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>100.0</far>\n    </clip>\n  </camera>\n  <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n    <frame_name>camera_frame</frame_name>\n    <topic_name>image_raw</topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"advanced-camera-properties",children:"Advanced Camera Properties"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="advanced_camera" type="camera">\n  <camera name="head">\n    \x3c!-- Lens distortion parameters --\x3e\n    <distortion>\n      <k1>0.0</k1>\n      <k2>0.0</k2>\n      <k3>0.0</k3>\n      <p1>0.0</p1>\n      <p2>0.0</p2>\n      <center>0.5 0.5</center>\n    </distortion>\n\n    \x3c!-- Noise model --\x3e\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.007</stddev>\n    </noise>\n  </camera>\n\n  \x3c!-- Output to ROS 2 topic --\x3e\n  <plugin name="camera_plugin" filename="libgazebo_ros_camera.so">\n    <frame_name>camera_optical_frame</frame_name>\n    <topic_name>camera/image_raw</topic_name>\n    <camera_info_topic_name>camera/camera_info</camera_info_topic_name>\n    <hack_baseline>0.07</hack_baseline>\n    <distortion_k1>0.0</distortion_k1>\n    <distortion_k2>0.0</distortion_k2>\n    <distortion_k3>0.0</distortion_k3>\n    <distortion_t1>0.0</distortion_t1>\n    <distortion_t2>0.0</distortion_t2>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"stereo-camera-configuration",children:"Stereo Camera Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Left camera --\x3e\n<sensor name="left_camera" type="camera">\n  <pose>0.05 0.0 0.0 0 0 0</pose>  \x3c!-- Offset from center --\x3e\n  <camera name="left">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image><width>640</width><height>480</height></image>\n  </camera>\n  <plugin name="left_camera" filename="libgazebo_ros_camera.so">\n    <frame_name>left_camera_frame</frame_name>\n    <topic_name>stereo/left/image_raw</topic_name>\n  </plugin>\n</sensor>\n\n\x3c!-- Right camera --\x3e\n<sensor name="right_camera" type="camera">\n  <pose>-0.05 0.0 0.0 0 0 0</pose>  \x3c!-- Offset from center --\x3e\n  <camera name="right">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image><width>640</width><height>480</height></image>\n  </camera>\n  <plugin name="right_camera" filename="libgazebo_ros_camera.so">\n    <frame_name>right_camera_frame</frame_name>\n    <topic_name>stereo/right/image_raw</topic_name>\n  </plugin>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"camera-integration-with-ros-2",children:"Camera Integration with ROS 2"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass CameraSimulator(Node):\n\n    def __init__(self):\n        super().__init__('camera_simulator')\n        self.bridge = CvBridge()\n\n        # Create publisher for camera image\n        self.image_pub = self.create_publisher(Image, '/camera/image_raw', 10)\n        self.info_pub = self.create_publisher(CameraInfo, '/camera/camera_info', 10)\n\n        # Timer to simulate image publishing\n        self.timer = self.create_timer(0.033, self.publish_camera_data)  # ~30 FPS\n\n    def publish_camera_data(self):\n        # In simulation, this data comes from Gazebo\n        # This is just an example of how to handle camera data\n\n        # Create a sample image (in real simulation, this comes from Gazebo)\n        sample_image = self.create_sample_image()\n\n        # Convert to ROS message\n        ros_image = self.bridge.cv2_to_imgmsg(sample_image, encoding='bgr8')\n        ros_image.header.stamp = self.get_clock().now().to_msg()\n        ros_image.header.frame_id = 'camera_optical_frame'\n\n        self.image_pub.publish(ros_image)\n\n        # Publish camera info\n        camera_info = self.create_camera_info()\n        camera_info.header.stamp = ros_image.header.stamp\n        camera_info.header.frame_id = 'camera_optical_frame'\n        self.info_pub.publish(camera_info)\n\n    def create_sample_image(self):\n        # Create a sample image for demonstration\n        import numpy as np\n        img = np.zeros((480, 640, 3), dtype=np.uint8)\n        cv2.rectangle(img, (100, 100), (200, 200), (0, 255, 0), -1)\n        return img\n\n    def create_camera_info(self):\n        info = CameraInfo()\n        info.width = 640\n        info.height = 480\n        info.k = [500.0, 0.0, 320.0,  # fx, 0, cx\n                  0.0, 500.0, 240.0,  # 0, fy, cy\n                  0.0, 0.0, 1.0]      # 0, 0, 1\n        return info\n"})}),"\n",(0,s.jsx)(e.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,s.jsx)(e.p,{children:"LiDAR (Light Detection and Ranging) sensors provide accurate 3D measurements of the environment. Simulated LiDAR must accurately model the physics of light reflection and the sensor's specific characteristics."}),"\n",(0,s.jsx)(e.h3,{id:"2d-lidar-configuration",children:"2D LiDAR Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="laser" type="ray">\n  <always_on>true</always_on>\n  <visualize>true</visualize>\n  <update_rate>10</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-1.570796</min_angle>  \x3c!-- -90 degrees --\x3e\n        <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="laser_plugin" filename="libgazebo_ros_laser.so">\n    <frame_name>laser_frame</frame_name>\n    <topic_name>scan</topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"3d-lidar-configuration-velodyne-style",children:"3D LiDAR Configuration (Velodyne-style)"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="velodyne" type="ray">\n  <pose>0 0 0.3 0 0 0</pose>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>800</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>32</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.5236</min_angle>  \x3c!-- -30 degrees --\x3e\n        <max_angle>0.2618</max_angle>   \x3c!-- 15 degrees --\x3e\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>100.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="velodyne_plugin" filename="libgazebo_ros_velodyne_laser.so">\n    <topic_name>velodyne_points</topic_name>\n    <frame_name>velodyne</frame_name>\n    <min_range>0.9</min_range>\n    <max_range>130.0</max_range>\n    <gaussian_noise>0.008</gaussian_noise>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"lidar-integration-with-ros-2",children:"LiDAR Integration with ROS 2"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom std_msgs.msg import Header\nimport math\n\nclass LidarSimulator(Node):\n\n    def __init__(self):\n        super().__init__('lidar_simulator')\n\n        # Create publisher for laser scan\n        self.scan_pub = self.create_publisher(LaserScan, '/scan', 10)\n        self.pc_pub = self.create_publisher(PointCloud2, '/points', 10)\n\n        # Timer to publish scan data\n        self.timer = self.create_timer(0.1, self.publish_scan_data)  # 10 Hz\n\n    def publish_scan_data(self):\n        scan = LaserScan()\n        scan.header = Header()\n        scan.header.stamp = self.get_clock().now().to_msg()\n        scan.header.frame_id = 'laser_frame'\n\n        # Configure scan parameters\n        scan.angle_min = -math.pi / 2  # -90 degrees\n        scan.angle_max = math.pi / 2   # 90 degrees\n        scan.angle_increment = math.pi / 180  # 1 degree\n        scan.time_increment = 0.0\n        scan.scan_time = 0.1\n        scan.range_min = 0.1\n        scan.range_max = 30.0\n\n        # Generate sample ranges (in real simulation, these come from Gazebo)\n        num_ranges = int((scan.angle_max - scan.angle_min) / scan.angle_increment) + 1\n        scan.ranges = [2.0 + 0.1 * i for i in range(num_ranges)]\n        scan.intensities = [100.0] * num_ranges  # Example intensities\n\n        self.scan_pub.publish(scan)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Inertial Measurement Units (IMUs) provide measurements of linear acceleration and angular velocity. Simulated IMUs must accurately model sensor noise, bias, and drift characteristics."}),"\n",(0,s.jsx)(e.h3,{id:"basic-imu-configuration",children:"Basic IMU Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>  \x3c!-- ~0.1 deg/s --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>  \x3c!-- 17 mg --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n    <frame_name>imu_link</frame_name>\n    <topic_name>imu</topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"advanced-imu-with-bias-and-drift",children:"Advanced IMU with Bias and Drift"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="advanced_imu" type="imu">\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>2.0e-4</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.001</bias_stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>2.0e-4</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.001</bias_stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>2.0e-4</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.001</bias_stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.017</bias_stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.017</bias_stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.017</bias_stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"imu-integration-with-ros-2",children:"IMU Integration with ROS 2"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nfrom std_msgs.msg import Header\nimport math\nimport numpy as np\n\nclass ImuSimulator(Node):\n\n    def __init__(self):\n        super().__init__('imu_simulator')\n\n        # Create publisher for IMU data\n        self.imu_pub = self.create_publisher(Imu, '/imu', 10)\n\n        # Timer to publish IMU data\n        self.timer = self.create_timer(0.01, self.publish_imu_data)  # 100 Hz\n\n        # Simulate IMU bias and drift\n        self.angular_velocity_bias = np.array([0.0, 0.0, 0.0])\n        self.linear_acceleration_bias = np.array([0.0, 0.0, 0.0])\n        self.time = 0.0\n\n    def publish_imu_data(self):\n        imu_msg = Imu()\n        imu_msg.header = Header()\n        imu_msg.header.stamp = self.get_clock().now().to_msg()\n        imu_msg.header.frame_id = 'imu_link'\n\n        # Simulate some motion (in real simulation, this comes from Gazebo physics)\n        self.time += 0.01\n\n        # Simulate angular velocity with noise\n        imu_msg.angular_velocity = Vector3()\n        imu_msg.angular_velocity.x = 0.1 * math.sin(self.time) + self.add_noise(0.0017)\n        imu_msg.angular_velocity.y = 0.05 * math.cos(self.time) + self.add_noise(0.0017)\n        imu_msg.angular_velocity.z = 0.02 * math.sin(2 * self.time) + self.add_noise(0.0017)\n\n        # Simulate linear acceleration with noise\n        imu_msg.linear_acceleration = Vector3()\n        imu_msg.linear_acceleration.x = 0.5 * math.cos(self.time) + self.add_noise(0.017)\n        imu_msg.linear_acceleration.y = 0.3 * math.sin(self.time) + self.add_noise(0.017)\n        imu_msg.linear_acceleration.z = 9.8 + 0.2 * math.sin(0.5 * self.time) + self.add_noise(0.017)\n\n        # For simplicity, we're not simulating orientation\n        # In a real system, this would come from integrating angular velocity\n        imu_msg.orientation.x = 0.0\n        imu_msg.orientation.y = 0.0\n        imu_msg.orientation.z = 0.0\n        imu_msg.orientation.w = 1.0\n\n        # Set covariance (diagonal elements only for simplicity)\n        imu_msg.angular_velocity_covariance = [0.0] * 9\n        imu_msg.linear_acceleration_covariance = [0.0] * 9\n        imu_msg.orientation_covariance = [0.0] * 9\n\n        self.imu_pub.publish(imu_msg)\n\n    def add_noise(self, stddev):\n        return np.random.normal(0, stddev)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"multi-sensor-fusion-simulation",children:"Multi-Sensor Fusion Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Real robots often use multiple sensors to improve perception. Here's how to simulate a multi-sensor setup:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Robot with multiple sensors --\x3e\n<model name="sensor_robot">\n  \x3c!-- Robot base --\x3e\n  <link name="base_link">\n    <inertial>...</inertial>\n    <visual>...</visual>\n    <collision>...</collision>\n  </link>\n\n  \x3c!-- Camera on robot --\x3e\n  <link name="camera_link">\n    <inertial>...</inertial>\n    <visual>...</visual>\n  </link>\n  <joint name="camera_joint" type="fixed">\n    <parent>base_link</parent>\n    <child>camera_link</child>\n    <origin xyz="0.1 0 0.1" rpy="0 0 0"/>\n  </joint>\n  <sensor name="front_camera" type="camera">...</sensor>\n\n  \x3c!-- LiDAR on robot --\x3e\n  <link name="laser_link">\n    <inertial>...</inertial>\n    <visual>...</visual>\n  </link>\n  <joint name="laser_joint" type="fixed">\n    <parent>base_link</parent>\n    <child>laser_link</child>\n    <origin xyz="0 0 0.2" rpy="0 0 0"/>\n  </joint>\n  <sensor name="front_laser" type="ray">...</sensor>\n\n  \x3c!-- IMU in robot body --\x3e\n  <sensor name="body_imu" type="imu">...</sensor>\n</model>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-accuracy-and-calibration",children:"Sensor Accuracy and Calibration"}),"\n",(0,s.jsx)(e.h3,{id:"modeling-sensor-imperfections",children:"Modeling Sensor Imperfections"}),"\n",(0,s.jsx)(e.p,{children:"Real sensors have various imperfections that should be modeled in simulation:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Camera with realistic distortion --\x3e\n<sensor name="realistic_camera" type="camera">\n  <camera name="head">\n    <distortion>\n      <k1>0.1</k1>      \x3c!-- Radial distortion --\x3e\n      <k2>-0.2</k2>     \x3c!-- Higher order radial distortion --\x3e\n      <k3>0.05</k3>     \x3c!-- Higher order radial distortion --\x3e\n      <p1>0.001</p1>    \x3c!-- Tangential distortion --\x3e\n      <p2>-0.001</p2>   \x3c!-- Tangential distortion --\x3e\n    </distortion>\n  </camera>\n</sensor>\n\n\x3c!-- LiDAR with realistic noise --\x3e\n<sensor name="realistic_laser" type="ray">\n  <ray>\n    <range>\n      <min>0.05</min>\n      <max>25.0</max>\n      <resolution>0.001</resolution>\n    </range>\n  </ray>\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>  \x3c!-- 1cm accuracy --\x3e\n  </noise>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"sensor-calibration-in-simulation",children:"Sensor Calibration in Simulation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class SensorCalibrator(Node):\n\n    def __init__(self):\n        super().__init__(\'sensor_calibrator\')\n\n        # Calibration parameters\n        self.camera_matrix = np.array([\n            [500.0, 0.0, 320.0],  # fx, 0, cx\n            [0.0, 500.0, 240.0],  # 0, fy, cy\n            [0.0, 0.0, 1.0]       # 0, 0, 1\n        ])\n\n        self.distortion_coeffs = np.array([0.1, -0.2, 0.001, -0.001, 0.05])  # k1, k2, p1, p2, k3\n\n    def undistort_image(self, image):\n        """Apply camera calibration to undistort image"""\n        undistorted = cv2.undistort(image, self.camera_matrix, self.distortion_coeffs)\n        return undistorted\n'})}),"\n",(0,s.jsx)(e.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,s.jsx)(e.h3,{id:"optimizing-sensor-simulation",children:"Optimizing Sensor Simulation"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Update Rates"}),": Balance accuracy with performance"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"High-rate sensors (IMU): 100-1000 Hz"}),"\n",(0,s.jsx)(e.li,{children:"Medium-rate sensors (LiDAR): 10-50 Hz"}),"\n",(0,s.jsx)(e.li,{children:"Low-rate sensors (cameras): 5-30 Hz"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Resolution"}),": Adjust sensor resolution based on requirements"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Higher resolution = more accurate but slower"}),"\n",(0,s.jsx)(e.li,{children:"Lower resolution = faster but less detailed"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Noise Modeling"}),": Include realistic noise without excessive computation"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Validate Against Real Sensors"}),": Compare simulated and real sensor data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Model Sensor Limitations"}),": Include realistic noise, range limits, and blind spots"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Calibrate Simulation"}),": Ensure simulated sensors match real hardware characteristics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Test Edge Cases"}),": Verify sensor behavior in challenging conditions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Document Parameters"}),": Keep track of all sensor simulation parameters"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"Sensor simulation is crucial for creating effective Digital Twin systems. By accurately modeling cameras, LiDAR, and IMUs with realistic characteristics, noise, and limitations, we can develop and test robot perception systems in simulation before deploying them on real hardware. The configurations and examples provided in this guide will help you create realistic sensor simulations that bridge the gap between digital intelligence and physical embodiment."})]})}function d(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>t,x:()=>o});var i=a(6540);const s={},r=i.createContext(s);function t(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);