"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[561],{6373:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-3-ai-brain/slam-localization","title":"Visual SLAM and Localization","description":"Introduction","source":"@site/docs/module-3-ai-brain/slam-localization.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/slam-localization","permalink":"/giaic-hackathon-speckit-plus/docs/module-3-ai-brain/slam-localization","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-ai-brain/slam-localization.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS Perception Pipelines","permalink":"/giaic-hackathon-speckit-plus/docs/module-3-ai-brain/perception-pipelines"},"next":{"title":"Navigation and Path Planning using Nav2","permalink":"/giaic-hackathon-speckit-plus/docs/module-3-ai-brain/navigation"}}');var t=a(4848),i=a(8453);const o={sidebar_position:8},r="Visual SLAM and Localization",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"SLAM Fundamentals",id:"slam-fundamentals",level:2},{value:"The SLAM Problem",id:"the-slam-problem",level:3},{value:"SLAM Mathematical Framework",id:"slam-mathematical-framework",level:3},{value:"Visual SLAM Approaches",id:"visual-slam-approaches",level:2},{value:"Feature-Based Visual SLAM",id:"feature-based-visual-slam",level:3},{value:"Direct Visual SLAM",id:"direct-visual-slam",level:3},{value:"Semi-Direct Methods",id:"semi-direct-methods",level:3},{value:"Isaac ROS Visual SLAM Implementation",id:"isaac-ros-visual-slam-implementation",level:2},{value:"Isaac ROS Stereo Visual Odometry",id:"isaac-ros-stereo-visual-odometry",level:3},{value:"Isaac ROS Loop Closure and Global Optimization",id:"isaac-ros-loop-closure-and-global-optimization",level:3},{value:"Localization Techniques",id:"localization-techniques",level:2},{value:"Monte Carlo Localization (Particle Filter)",id:"monte-carlo-localization-particle-filter",level:3},{value:"Extended Kalman Filter (EKF) Localization",id:"extended-kalman-filter-ekf-localization",level:3},{value:"Mapping Techniques",id:"mapping-techniques",level:2},{value:"Occupancy Grid Mapping",id:"occupancy-grid-mapping",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Multi-Threaded SLAM Architecture",id:"multi-threaded-slam-architecture",level:3},{value:"Isaac ROS Integration",id:"isaac-ros-integration",level:2},{value:"Launch Configuration for Visual SLAM",id:"launch-configuration-for-visual-slam",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. Drift in Visual Odometry",id:"1-drift-in-visual-odometry",level:3},{value:"2. Feature Loss in Textureless Environments",id:"2-feature-loss-in-textureless-environments",level:3},{value:"3. Scale Ambiguity in Monocular SLAM",id:"3-scale-ambiguity-in-monocular-slam",level:3},{value:"4. Real-time Performance Issues",id:"4-real-time-performance-issues",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"visual-slam-and-localization",children:"Visual SLAM and Localization"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Simultaneous Localization and Mapping (SLAM) is a fundamental capability for autonomous robots, enabling them to build a map of an unknown environment while simultaneously determining their position within that map. Visual SLAM specifically uses camera sensors to extract features and landmarks for mapping and localization. This guide covers Visual SLAM concepts, algorithms, and implementation using Isaac ROS and related tools."}),"\n",(0,t.jsx)(n.h2,{id:"slam-fundamentals",children:"SLAM Fundamentals"}),"\n",(0,t.jsx)(n.h3,{id:"the-slam-problem",children:"The SLAM Problem"}),"\n",(0,t.jsx)(n.p,{children:'SLAM addresses the "chicken and egg" problem in robotics: to build a map, you need to know where you are, but to know where you are, you need a map. The SLAM solution jointly estimates:'}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robot Trajectory"}),": The sequence of robot poses over time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Map"}),": The locations of landmarks or features in the environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Current State"}),": The robot's current pose relative to the map"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"slam-mathematical-framework",children:"SLAM Mathematical Framework"}),"\n",(0,t.jsx)(n.p,{children:"The SLAM problem can be formulated as a probabilistic estimation problem:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"P(x_t, m | z_1:t, u_1:t)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"x_t"})," is the robot trajectory up to time t"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"m"})," is the map"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"z_1:t"})," is the sequence of observations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"u_1:t"})," is the sequence of control inputs"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"visual-slam-approaches",children:"Visual SLAM Approaches"}),"\n",(0,t.jsx)(n.h3,{id:"feature-based-visual-slam",children:"Feature-Based Visual SLAM"}),"\n",(0,t.jsx)(n.p,{children:"Feature-based methods extract and track distinctive features (points, lines, corners) in the environment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass FeatureBasedVisualSLAM:\n    def __init__(self):\n        # Feature detector and descriptor\n        self.detector = cv2.SIFT_create()\n        self.matcher = cv2.BFMatcher()\n\n        # Camera parameters\n        self.K = np.array([\n            [615.0, 0.0, 320.0],\n            [0.0, 615.0, 240.0],\n            [0.0, 0.0, 1.0]\n        ])\n\n        # Pose tracking\n        self.current_pose = np.eye(4)\n        self.keyframes = []\n        self.map_points = []\n\n    def process_frame(self, image):\n        """Process a new frame for SLAM"""\n        # Detect features\n        keypoints, descriptors = self.detector.detectAndCompute(image, None)\n\n        if len(keypoints) < 100:  # Need sufficient features\n            return self.current_pose\n\n        # Track features from previous frame\n        if hasattr(self, \'prev_keypoints\') and self.prev_descriptors is not None:\n            matches = self.matcher.knnMatch(\n                descriptors, self.prev_descriptors, k=2\n            )\n\n            # Apply Lowe\'s ratio test\n            good_matches = []\n            for match_pair in matches:\n                if len(match_pair) == 2:\n                    m, n = match_pair\n                    if m.distance < 0.7 * n.distance:\n                        good_matches.append(m)\n\n            if len(good_matches) >= 10:  # Need sufficient matches\n                # Extract matched points\n                prev_pts = np.float32([\n                    self.prev_keypoints[m.trainIdx].pt for m in good_matches\n                ]).reshape(-1, 1, 2)\n                curr_pts = np.float32([\n                    keypoints[m.queryIdx].pt for m in good_matches\n                ]).reshape(-1, 1, 2)\n\n                # Estimate essential matrix\n                E, mask = cv2.findEssentialMat(\n                    curr_pts, prev_pts, self.K,\n                    method=cv2.RANSAC, threshold=1.0\n                )\n\n                if E is not None:\n                    # Recover pose\n                    _, R, t, _ = cv2.recoverPose(E, curr_pts, prev_pts, self.K)\n\n                    # Update current pose\n                    T = np.eye(4)\n                    T[:3, :3] = R\n                    T[:3, 3] = t.flatten()\n                    self.current_pose = self.current_pose @ np.linalg.inv(T)\n\n        # Store current frame for next iteration\n        self.prev_keypoints = keypoints\n        self.prev_descriptors = descriptors\n\n        return self.current_pose\n'})}),"\n",(0,t.jsx)(n.h3,{id:"direct-visual-slam",children:"Direct Visual SLAM"}),"\n",(0,t.jsx)(n.p,{children:"Direct methods use pixel intensities directly rather than extracting features:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class DirectVisualSLAM:\n    def __init__(self):\n        self.reference_frame = None\n        self.depth_map = None\n        self.current_pose = np.eye(4)\n\n    def estimate_motion_direct(self, current_frame, reference_frame, K):\n        """Estimate motion using direct method"""\n        # Compute image gradients\n        grad_x = cv2.Sobel(reference_frame, cv2.CV_64F, 1, 0, ksize=3)\n        grad_y = cv2.Sobel(reference_frame, cv2.CV_64F, 0, 1, ksize=3)\n\n        # Initialize pose increment\n        xi = np.zeros(6)  # [rx, ry, rz, tx, ty, tz]\n\n        # Iterative optimization (simplified)\n        for iteration in range(10):\n            # Warp current frame based on current estimate\n            warped_frame = self.warp_frame(\n                current_frame, reference_frame, self.depth_map,\n                self.current_pose, K, xi\n            )\n\n            # Compute photometric error\n            error = reference_frame.astype(float) - warped_frame.astype(float)\n\n            # Compute Jacobian (simplified)\n            # In practice, this would involve more complex derivations\n            jacobian = self.compute_jacobian(grad_x, grad_y, self.depth_map)\n\n            # Update pose estimate\n            delta_xi = np.linalg.lstsq(jacobian, error.flatten(), rcond=None)[0]\n            xi += delta_xi\n\n        # Update pose\n        self.current_pose = self.pose_vector_to_matrix(xi) @ self.current_pose\n\n        return self.current_pose\n\n    def warp_frame(self, current_frame, reference_frame, depth_map, pose, K, xi):\n        """Warp current frame to reference frame"""\n        # Implementation details for frame warping\n        # This is a simplified placeholder\n        return current_frame\n\n    def compute_jacobian(self, grad_x, grad_y, depth_map):\n        """Compute Jacobian matrix for direct method"""\n        # Implementation details\n        return np.eye(6)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"semi-direct-methods",children:"Semi-Direct Methods"}),"\n",(0,t.jsx)(n.p,{children:"Semi-direct methods combine feature tracking with direct alignment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SemiDirectSLAM:\n    def __init__(self):\n        # Feature tracking for robust matching\n        self.feature_tracker = FeatureBasedVisualSLAM()\n\n        # Direct alignment for precision\n        self.direct_aligner = DirectVisualSLAM()\n\n        # Map management\n        self.map = Map()\n        self.local_window = []\n\n    def process_frame(self, image):\n        """Process frame using semi-direct approach"""\n        # Step 1: Track features for robust initialization\n        tracked_pose = self.feature_tracker.process_frame(image)\n\n        # Step 2: Refine using direct alignment\n        refined_pose = self.direct_aligner.estimate_motion_direct(\n            image, self.get_reference_frame(), self.feature_tracker.K\n        )\n\n        # Step 3: Update map and local window\n        self.update_map(image, refined_pose)\n\n        return refined_pose\n\n    def update_map(self, image, pose):\n        """Update map with new observations"""\n        # Add keyframe if significant motion detected\n        if self.is_keyframe_needed(pose):\n            self.add_keyframe(image, pose)\n\n        # Optimize local window\n        self.optimize_local_window()\n\n    def is_keyframe_needed(self, pose):\n        """Determine if current frame should be a keyframe"""\n        # Check for sufficient translation/rotation\n        if len(self.local_window) == 0:\n            return True\n\n        last_pose = self.local_window[-1].pose\n        delta_t = np.linalg.norm(pose[:3, 3] - last_pose[:3, 3])\n        delta_r = R.from_matrix(pose[:3, :3] @ last_pose[:3, :3].T).as_rotvec()\n\n        return delta_t > 0.1 or np.linalg.norm(delta_r) > 0.1\n'})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-visual-slam-implementation",children:"Isaac ROS Visual SLAM Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-stereo-visual-odometry",children:"Isaac ROS Stereo Visual Odometry"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS provides optimized stereo visual odometry nodes:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacStereoOdometryNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_stereo_odometry\')\n\n        # Subscribe to stereo images\n        self.left_sub = self.create_subscription(\n            Image,\n            \'/stereo/left/image_rect\',\n            self.left_callback,\n            10\n        )\n\n        self.right_sub = self.create_subscription(\n            Image,\n            \'/stereo/right/image_rect\',\n            self.right_callback,\n            10\n        )\n\n        # Publish odometry\n        self.odom_pub = self.create_publisher(Odometry, \'/visual_odom\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/visual_pose\', 10)\n\n        self.bridge = CvBridge()\n\n        # Initialize stereo odometry\n        self.left_image = None\n        self.right_image = None\n        self.initialized = False\n        self.current_pose = np.eye(4)\n        self.prev_pose = np.eye(4)\n\n        # Camera parameters (should be loaded from calibration)\n        self.camera_matrix = np.array([\n            [615.0, 0.0, 320.0],\n            [0.0, 615.0, 240.0],\n            [0.0, 0.0, 1.0]\n        ])\n\n        # Stereo baseline (distance between cameras)\n        self.baseline = 0.1  # 10 cm\n\n    def left_callback(self, msg):\n        """Process left camera image"""\n        if not self.initialized:\n            self.initialize_stereo_odometry(msg)\n            return\n\n        self.process_stereo_odometry(msg, self.right_image)\n\n    def right_callback(self, msg):\n        """Process right camera image"""\n        self.right_image = self.bridge.imgmsg_to_cv2(msg, "mono8")\n\n    def initialize_stereo_odometry(self, left_msg):\n        """Initialize stereo odometry with first frame"""\n        self.left_image = self.bridge.imgmsg_to_cv2(left_msg, "mono8")\n        self.initialized = True\n        self.get_logger().info("Stereo odometry initialized")\n\n    def process_stereo_odometry(self, left_msg, right_img):\n        """Process stereo images for odometry"""\n        if right_img is None:\n            return\n\n        left_img = self.bridge.imgmsg_to_cv2(left_msg, "mono8")\n\n        try:\n            # Extract features from left image\n            kp1, desc1 = self.extract_features(left_img)\n\n            # Extract features from previous left image\n            kp2, desc2 = self.extract_features(self.left_image)\n\n            # Match features\n            matches = self.match_features(desc1, desc2)\n\n            if len(matches) >= 10:\n                # Estimate motion\n                motion = self.estimate_motion(kp1, kp2, matches)\n\n                # Update pose\n                self.prev_pose = self.current_pose.copy()\n                self.current_pose = self.current_pose @ motion\n\n                # Publish odometry\n                self.publish_odometry(left_msg.header)\n\n            # Update previous image\n            self.left_image = left_img\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in stereo odometry: {e}\')\n\n    def extract_features(self, image):\n        """Extract SIFT features from image"""\n        detector = cv2.SIFT_create(nfeatures=1000)\n        kp, desc = detector.detectAndCompute(image, None)\n        return kp, desc\n\n    def match_features(self, desc1, desc2):\n        """Match features between two descriptors"""\n        if desc1 is None or desc2 is None:\n            return []\n\n        bf = cv2.BFMatcher()\n        matches = bf.knnMatch(desc1, desc2, k=2)\n\n        # Apply Lowe\'s ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.7 * n.distance:\n                    good_matches.append(m)\n\n        return good_matches\n\n    def estimate_motion(self, kp1, kp2, matches):\n        """Estimate 3D motion from matched features"""\n        if len(matches) < 10:\n            return np.eye(4)\n\n        # Get matched points\n        pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n        pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n        # Estimate fundamental matrix\n        F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.RANSAC, 4, 0.999)\n        if F is None:\n            return np.eye(4)\n\n        # Estimate essential matrix\n        E = self.camera_matrix.T @ F @ self.camera_matrix\n\n        # Recover pose\n        _, R, t, _ = cv2.recoverPose(E, pts1, pts2, self.camera_matrix)\n\n        # Create transformation matrix\n        T = np.eye(4)\n        T[:3, :3] = R\n        T[:3, 3] = t.flatten()\n\n        return T\n\n    def publish_odometry(self, header):\n        """Publish odometry message"""\n        odom_msg = Odometry()\n        odom_msg.header = header\n        odom_msg.header.frame_id = "map"\n        odom_msg.child_frame_id = "camera"\n\n        # Convert pose to ROS format\n        pose = self.current_pose\n        odom_msg.pose.pose.position.x = pose[0, 3]\n        odom_msg.pose.pose.position.y = pose[1, 3]\n        odom_msg.pose.pose.position.z = pose[2, 3]\n\n        # Convert rotation matrix to quaternion\n        r = R.from_matrix(pose[:3, :3])\n        quat = r.as_quat()\n        odom_msg.pose.pose.orientation.x = quat[0]\n        odom_msg.pose.pose.orientation.y = quat[1]\n        odom_msg.pose.pose.orientation.z = quat[2]\n        odom_msg.pose.pose.orientation.w = quat[3]\n\n        # Velocity would be computed from pose differences\n        # For now, set to zero\n        odom_msg.twist.twist.linear.x = 0.0\n        odom_msg.twist.twist.linear.y = 0.0\n        odom_msg.twist.twist.linear.z = 0.0\n        odom_msg.twist.twist.angular.x = 0.0\n        odom_msg.twist.twist.angular.y = 0.0\n        odom_msg.twist.twist.angular.z = 0.0\n\n        self.odom_pub.publish(odom_msg)\n\n        # Also publish as PoseStamped\n        pose_msg = PoseStamped()\n        pose_msg.header = header\n        pose_msg.header.frame_id = "map"\n        pose_msg.pose = odom_msg.pose.pose\n        self.pose_pub.publish(pose_msg)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-loop-closure-and-global-optimization",children:"Isaac ROS Loop Closure and Global Optimization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseArray, PoseWithCovarianceStamped\nfrom std_msgs.msg import Bool\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\nclass IsaacLoopClosureNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_loop_closure\')\n\n        # Subscribe to pose estimates\n        self.pose_sub = self.create_subscription(\n            PoseWithCovarianceStamped,\n            \'/amcl_pose\',  # or visual odometry pose\n            self.pose_callback,\n            10\n        )\n\n        # Publish corrected poses\n        self.corrected_pose_pub = self.create_publisher(\n            PoseWithCovarianceStamped,\n            \'/corrected_pose\',\n            10\n        )\n\n        # Loop closure detection\n        self.pose_history = []\n        self.loop_threshold = 2.0  # meters\n        self.min_loop_features = 50\n\n        # Graph optimization parameters\n        self.optimization_queue = []\n        self.optimization_threshold = 10  # poses before optimization\n\n    def pose_callback(self, msg):\n        """Process incoming pose estimates"""\n        # Store pose in history\n        pose_data = {\n            \'timestamp\': msg.header.stamp,\n            \'pose\': msg.pose.pose,\n            \'position\': np.array([\n                msg.pose.pose.position.x,\n                msg.pose.pose.position.y,\n                msg.pose.pose.position.z\n            ]),\n            \'features\': self.extract_local_features(msg)  # Simplified\n        }\n\n        self.pose_history.append(pose_data)\n\n        # Check for potential loop closures\n        loop_candidates = self.find_loop_candidates(pose_data)\n\n        if loop_candidates:\n            # Verify loop closure\n            confirmed_loops = self.verify_loop_closure(pose_data, loop_candidates)\n\n            if confirmed_loops:\n                # Optimize graph\n                self.optimize_graph(confirmed_loops)\n\n    def find_loop_candidates(self, current_pose_data):\n        """Find potential loop closure candidates"""\n        candidates = []\n\n        # Simple distance-based search\n        current_pos = current_pose_data[\'position\']\n\n        for i, past_pose in enumerate(self.pose_history[:-10]):  # Skip recent poses\n            distance = np.linalg.norm(current_pos - past_pose[\'position\'])\n\n            if distance < self.loop_threshold:\n                candidates.append(i)\n\n        return candidates\n\n    def verify_loop_closure(self, current_pose_data, candidates):\n        """Verify potential loop closures using feature matching"""\n        confirmed = []\n\n        for idx in candidates:\n            past_pose = self.pose_history[idx]\n\n            # Match features between current and past pose\n            if self.match_features(current_pose_data[\'features\'], past_pose[\'features\']):\n                confirmed.append(idx)\n\n        return confirmed\n\n    def match_features(self, features1, features2):\n        """Match features between two sets (simplified)"""\n        # In practice, this would use more sophisticated feature matching\n        # For now, just check if we have enough features\n        return (len(features1) > self.min_loop_features and\n                len(features2) > self.min_loop_features)\n\n    def optimize_graph(self, loop_constraints):\n        """Optimize pose graph with loop closure constraints"""\n        # This is a simplified placeholder\n        # In practice, would use libraries like g2o or Ceres Solver\n\n        # For demonstration, just adjust poses based on constraints\n        for loop_idx in loop_constraints:\n            # Adjust poses to minimize drift\n            self.adjust_poses_for_loop_closure(loop_idx)\n\n    def extract_local_features(self, pose_msg):\n        """Extract local features for loop closure detection"""\n        # In practice, this would extract visual features from camera\n        # For now, return a simple descriptor\n        return np.array([\n            pose_msg.pose.position.x,\n            pose_msg.pose.position.y,\n            pose_msg.pose.orientation.w  # Simplified\n        ])\n'})}),"\n",(0,t.jsx)(n.h2,{id:"localization-techniques",children:"Localization Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"monte-carlo-localization-particle-filter",children:"Monte Carlo Localization (Particle Filter)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy.stats import norm\nfrom geometry_msgs.msg import PoseArray, Pose\n\nclass ParticleFilterLocalization:\n    def __init__(self, map_resolution=0.05, map_size=(200, 200)):\n        self.map_resolution = map_resolution\n        self.map_size = map_size\n        self.map = np.zeros(map_size)  # Occupancy grid map\n\n        # Particle filter parameters\n        self.num_particles = 1000\n        self.particles = np.zeros((self.num_particles, 3))  # x, y, theta\n        self.weights = np.ones(self.num_particles) / self.num_particles\n\n        # Motion model noise\n        self.motion_noise = [0.1, 0.1, 0.05]  # x, y, theta\n\n        # Sensor model parameters\n        self.sensor_std = 0.1\n\n    def initialize_particles_uniform(self):\n        """Initialize particles uniformly across the map"""\n        for i in range(self.num_particles):\n            x = np.random.uniform(0, self.map_size[0] * self.map_resolution)\n            y = np.random.uniform(0, self.map_size[1] * self.map_resolution)\n            theta = np.random.uniform(-np.pi, np.pi)\n            self.particles[i] = [x, y, theta]\n\n    def predict(self, control):\n        """Predict particle motion based on control input"""\n        # Add noise to control input\n        noisy_control = [\n            control[0] + np.random.normal(0, self.motion_noise[0]),\n            control[1] + np.random.normal(0, self.motion_noise[1]),\n            control[2] + np.random.normal(0, self.motion_noise[2])\n        ]\n\n        # Update particle poses\n        for i in range(self.num_particles):\n            self.particles[i, 0] += noisy_control[0] * np.cos(self.particles[i, 2])\n            self.particles[i, 1] += noisy_control[0] * np.sin(self.particles[i, 2])\n            self.particles[i, 2] += noisy_control[2]\n\n            # Normalize angle\n            self.particles[i, 2] = self.normalize_angle(self.particles[i, 2])\n\n    def update(self, observations):\n        """Update particle weights based on sensor observations"""\n        for i in range(self.num_particles):\n            weight = 1.0\n\n            for obs in observations:\n                # Predict what this particle expects to see\n                expected_obs = self.predict_observation(self.particles[i])\n\n                # Calculate likelihood of actual observation\n                likelihood = self.calculate_likelihood(obs, expected_obs)\n                weight *= likelihood\n\n            self.weights[i] = weight\n\n        # Normalize weights\n        self.weights += 1.e-300  # Avoid numerical issues\n        self.weights /= np.sum(self.weights)\n\n        # Resample if effective sample size is too low\n        if self.effective_sample_size() < self.num_particles / 2:\n            self.resample()\n\n    def predict_observation(self, particle_pose):\n        """Predict sensor observations for a given particle pose"""\n        # This would typically involve ray-casting or other sensor models\n        # Simplified for demonstration\n        return np.array([particle_pose[0], particle_pose[1]])\n\n    def calculate_likelihood(self, actual, expected):\n        """Calculate likelihood of observation"""\n        diff = actual - expected\n        distance = np.linalg.norm(diff)\n        return norm.pdf(distance, 0, self.sensor_std)\n\n    def effective_sample_size(self):\n        """Calculate effective sample size"""\n        return 1.0 / np.sum(self.weights ** 2)\n\n    def resample(self):\n        """Resample particles based on weights"""\n        indices = np.random.choice(\n            self.num_particles,\n            size=self.num_particles,\n            p=self.weights\n        )\n\n        self.particles = self.particles[indices]\n        self.weights.fill(1.0 / self.num_particles)\n\n    def estimate_pose(self):\n        """Estimate robot pose from particles"""\n        # Weighted average of particles\n        x = np.average(self.particles[:, 0], weights=self.weights)\n        y = np.average(self.particles[:, 1], weights=self.weights)\n\n        # For angle, need to handle circular statistics\n        cos_sum = np.average(np.cos(self.particles[:, 2]), weights=self.weights)\n        sin_sum = np.average(np.sin(self.particles[:, 2]), weights=self.weights)\n        theta = np.arctan2(sin_sum, cos_sum)\n\n        return np.array([x, y, theta])\n\n    def normalize_angle(self, angle):\n        """Normalize angle to [-\u03c0, \u03c0] range"""\n        while angle > np.pi:\n            angle -= 2 * np.pi\n        while angle < -np.pi:\n            angle += 2 * np.pi\n        return angle\n'})}),"\n",(0,t.jsx)(n.h3,{id:"extended-kalman-filter-ekf-localization",children:"Extended Kalman Filter (EKF) Localization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class EKFLocalization:\n    def __init__(self):\n        # State: [x, y, theta, vx, vy, omega]\n        self.state = np.zeros(6)\n        self.covariance = np.eye(6) * 0.1\n\n        # Process noise\n        self.Q = np.diag([0.1, 0.1, 0.01, 0.1, 0.1, 0.01])\n\n        # Measurement noise (for different sensor types)\n        self.R_landmark = np.diag([0.1, 0.1])  # range, bearing\n        self.R_odometry = np.diag([0.05, 0.05, 0.01])  # x, y, theta\n\n    def predict(self, control, dt):\n        """Predict state and covariance"""\n        # Motion model: constant velocity\n        F = self.compute_jacobian_motion()\n\n        # Update state\n        self.state[0] += self.state[3] * dt  # x = x + vx * dt\n        self.state[1] += self.state[4] * dt  # y = y + vy * dt\n        self.state[2] += self.state[5] * dt  # theta = theta + omega * dt\n\n        # Update covariance\n        self.covariance = F @ self.covariance @ F.T + self.Q\n\n    def update_landmark(self, landmark_pos, observed_range, observed_bearing):\n        """Update with landmark observation"""\n        # Predicted measurement\n        dx = landmark_pos[0] - self.state[0]\n        dy = landmark_pos[1] - self.state[1]\n        predicted_range = np.sqrt(dx**2 + dy**2)\n        predicted_bearing = np.arctan2(dy, dx) - self.state[2]\n\n        # Measurement model Jacobian\n        H = self.compute_jacobian_landmark(landmark_pos)\n\n        # Innovation\n        innovation = np.array([\n            observed_range - predicted_range,\n            self.normalize_angle(observed_bearing - predicted_bearing)\n        ])\n\n        # Innovation covariance\n        S = H @ self.covariance @ H.T + self.R_landmark\n\n        # Kalman gain\n        K = self.covariance @ H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        self.state += K @ innovation\n        self.covariance = (np.eye(len(self.state)) - K @ H) @ self.covariance\n\n    def compute_jacobian_motion(self):\n        """Compute motion model Jacobian"""\n        dt = 0.1  # time step\n        F = np.eye(6)\n        F[0, 3] = dt  # dx/dvx\n        F[1, 4] = dt  # dy/dvy\n        F[2, 5] = dt  # dtheta/domega\n        return F\n\n    def compute_jacobian_landmark(self, landmark_pos):\n        """Compute landmark measurement Jacobian"""\n        dx = landmark_pos[0] - self.state[0]\n        dy = landmark_pos[1] - self.state[1]\n        q = dx**2 + dy**2\n        sqrt_q = np.sqrt(q)\n\n        H = np.zeros((2, 6))\n        H[0, 0] = -dx / sqrt_q  # dr/dx\n        H[0, 1] = -dy / sqrt_q  # dr/dy\n        H[1, 0] = dy / q        # db/dx\n        H[1, 1] = -dx / q       # db/dy\n        H[1, 2] = -1            # db/dtheta\n\n        return H\n\n    def normalize_angle(self, angle):\n        """Normalize angle to [-\u03c0, \u03c0] range"""\n        while angle > np.pi:\n            angle -= 2 * np.pi\n        while angle < -np.pi:\n            angle += 2 * np.pi\n        return angle\n'})}),"\n",(0,t.jsx)(n.h2,{id:"mapping-techniques",children:"Mapping Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"occupancy-grid-mapping",children:"Occupancy Grid Mapping"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class OccupancyGridMapper:\n    def __init__(self, resolution=0.05, width=20, height=20):\n        self.resolution = resolution\n        self.width = width\n        self.height = height\n        self.grid = np.zeros((int(height/resolution), int(width/resolution)))\n\n        # Log-odds representation\n        self.log_odds = np.zeros_like(self.grid)\n        self.occupied_threshold = 0.5\n\n        # Sensor parameters\n        self.max_range = 10.0\n        self.angle_increment = 0.01\n\n    def update_map(self, scan_data, robot_pose):\n        """Update occupancy grid with laser scan data"""\n        # Convert scan to world coordinates\n        world_points = self.scan_to_world(scan_data, robot_pose)\n\n        # Get free space points (endpoints of valid rays)\n        endpoints = self.get_valid_endpoints(scan_data, robot_pose)\n\n        # Update grid with hits and misses\n        for point in world_points:\n            grid_x, grid_y = self.world_to_grid(point)\n            if self.is_valid_grid_cell(grid_x, grid_y):\n                self.log_odds[grid_y, grid_x] += self.log_odds_occupied\n\n        for endpoint in endpoints:\n            grid_x, grid_y = self.world_to_grid(endpoint)\n            if self.is_valid_grid_cell(grid_x, grid_y):\n                self.log_odds[grid_y, grid_x] += self.log_odds_free\n\n        # Convert back to probability\n        self.grid = self.log_odds_to_probability(self.log_odds)\n\n    def scan_to_world(self, scan_data, robot_pose):\n        """Convert laser scan to world coordinates"""\n        points = []\n        robot_x, robot_y, robot_theta = robot_pose\n\n        for i, range_reading in enumerate(scan_data.ranges):\n            if 0 < range_reading < self.max_range:\n                angle = scan_data.angle_min + i * scan_data.angle_increment\n                world_x = robot_x + range_reading * np.cos(robot_theta + angle)\n                world_y = robot_y + range_reading * np.sin(robot_theta + angle)\n                points.append([world_x, world_y])\n\n        return points\n\n    def get_valid_endpoints(self, scan_data, robot_pose):\n        """Get endpoints of valid laser rays"""\n        endpoints = []\n        robot_x, robot_y, robot_theta = robot_pose\n\n        for i, range_reading in enumerate(scan_data.ranges):\n            if 0 < range_reading < self.max_range:\n                angle = scan_data.angle_min + i * scan_data.angle_increment\n                world_x = robot_x + range_reading * np.cos(robot_theta + angle)\n                world_y = robot_y + range_reading * np.sin(robot_theta + angle)\n                endpoints.append([world_x, world_y])\n\n        return endpoints\n\n    def world_to_grid(self, world_point):\n        """Convert world coordinates to grid indices"""\n        grid_x = int((world_point[0] + self.width/2) / self.resolution)\n        grid_y = int((world_point[1] + self.height/2) / self.resolution)\n        return grid_x, grid_y\n\n    def is_valid_grid_cell(self, x, y):\n        """Check if grid coordinates are valid"""\n        return (0 <= x < self.grid.shape[1]) and (0 <= y < self.grid.shape[0])\n\n    def log_odds_to_probability(self, log_odds):\n        """Convert log-odds to probability"""\n        prob = 1 - 1 / (1 + np.exp(log_odds))\n        return prob\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"multi-threaded-slam-architecture",children:"Multi-Threaded SLAM Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import threading\nimport queue\nimport time\n\nclass MultiThreadedSLAM:\n    def __init__(self):\n        # Queues for inter-thread communication\n        self.image_queue = queue.Queue(maxsize=5)\n        self.feature_queue = queue.Queue(maxsize=10)\n        self.optimization_queue = queue.Queue(maxsize=5)\n\n        # Threading locks\n        self.map_lock = threading.Lock()\n\n        # Worker threads\n        self.feature_thread = threading.Thread(target=self.feature_extraction_worker)\n        self.optimization_thread = threading.Thread(target=self.optimization_worker)\n        self.mapping_thread = threading.Thread(target=self.mapping_worker)\n\n        # Start threads\n        self.feature_thread.start()\n        self.optimization_thread.start()\n        self.mapping_thread.start()\n\n    def process_image(self, image):\n        """Main entry point for image processing"""\n        try:\n            self.image_queue.put_nowait(image)\n        except queue.Full:\n            # Drop frame if queue is full\n            pass\n\n    def feature_extraction_worker(self):\n        """Extract features from images in background"""\n        while True:\n            try:\n                image = self.image_queue.get(timeout=1.0)\n\n                # Extract features\n                features = self.extract_features(image)\n\n                # Add to feature queue\n                self.feature_queue.put((image, features))\n\n            except queue.Empty:\n                continue\n\n    def optimization_worker(self):\n        """Run graph optimization in background"""\n        while True:\n            try:\n                optimization_data = self.optimization_queue.get(timeout=1.0)\n\n                # Perform optimization\n                optimized_poses = self.optimize_graph(optimization_data)\n\n                # Update global map\n                with self.map_lock:\n                    self.update_global_map(optimized_poses)\n\n            except queue.Empty:\n                continue\n\n    def mapping_worker(self):\n        """Build map from features and poses"""\n        while True:\n            try:\n                image, features = self.feature_queue.get(timeout=1.0)\n\n                # Estimate pose and update map\n                pose = self.estimate_pose(features)\n                self.update_local_map(image, pose)\n\n                # Add to optimization queue if needed\n                if self.should_optimize():\n                    optimization_data = self.prepare_optimization_data()\n                    self.optimization_queue.put(optimization_data)\n\n            except queue.Empty:\n                continue\n'})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-integration",children:"Isaac ROS Integration"}),"\n",(0,t.jsx)(n.h3,{id:"launch-configuration-for-visual-slam",children:"Launch Configuration for Visual SLAM"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example launch file for Isaac ROS Visual SLAM --\x3e\n<launch>\n  \x3c!-- Stereo camera drivers --\x3e\n  <node pkg="isaac_ros_stereo_image_proc" exec="isaac_ros_stereo_rectify" name="stereo_rectify">\n    <param name="left_topic" value="/left/image_raw"/>\n    <param name="right_topic" value="/right/image_raw"/>\n    <param name="left_camera_info_url" value="file://$(find-pkg-share my_robot_description)/config/left_camera.yaml"/>\n    <param name="right_camera_info_url" value="file://$(find-pkg-share my_robot_description)/config/right_camera.yaml"/>\n  </node>\n\n  \x3c!-- Visual odometry --\x3e\n  <node pkg="isaac_ros_visual_odometry" exec="isaac_ros_visual_odometry" name="visual_odometry">\n    <param name="left_rect_topic" value="/left/image_rect"/>\n    <param name="right_rect_topic" value="/right/image_rect"/>\n    <param name="max_num_features" value="1000"/>\n    <param name="min_num_features" value="100"/>\n  </node>\n\n  \x3c!-- Loop closure detection --\x3e\n  <node pkg="isaac_ros_loop_closure" exec="isaac_ros_loop_closure" name="loop_closure">\n    <param name="pose_topic" value="/visual_odom"/>\n    <param name="loop_threshold" value="2.0"/>\n  </node>\n\n  \x3c!-- Map building --\x3e\n  <node pkg="isaac_ros_map_building" exec="isaac_ros_map_builder" name="map_builder">\n    <param name="submap_resolution" value="0.05"/>\n    <param name="map_publish_period" value="1.0"/>\n  </node>\n\n  \x3c!-- Localization --\x3e\n  <node pkg="isaac_ros_localization" exec="isaac_ros_localization" name="localization">\n    <param name="initial_pose_topic" value="/initialpose"/>\n    <param name="map_topic" value="/map"/>\n  </node>\n</launch>\n'})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,t.jsx)(n.h3,{id:"1-drift-in-visual-odometry",children:"1. Drift in Visual Odometry"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem"}),": Accumulating position error over time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Implement loop closure, use sensor fusion with IMU, optimize feature tracking"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-feature-loss-in-textureless-environments",children:"2. Feature Loss in Textureless Environments"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem"}),": Insufficient features for tracking"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Use direct methods, combine with other sensors, implement featureless SLAM"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-scale-ambiguity-in-monocular-slam",children:"3. Scale Ambiguity in Monocular SLAM"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem"}),": Cannot determine absolute scale from single camera"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Use stereo cameras, add IMU data, use known object sizes"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-real-time-performance-issues",children:"4. Real-time Performance Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem"}),": SLAM pipeline too slow for real-time operation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Optimize algorithms, reduce feature count, use GPU acceleration"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Visual SLAM and localization are critical capabilities for autonomous robots, enabling them to navigate and operate in unknown environments. This guide covered the fundamental concepts, algorithms, and implementation approaches for visual SLAM, including feature-based methods, direct methods, and Isaac ROS integration. Proper implementation of SLAM systems requires careful attention to sensor calibration, algorithm optimization, and real-time performance considerations. The combination of visual perception and robust localization forms the foundation for intelligent robot navigation and mapping systems."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>r});var s=a(6540);const t={},i=s.createContext(t);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);