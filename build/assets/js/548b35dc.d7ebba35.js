"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[441],{3207:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2-digital-twin/human-robot-interaction","title":"Human-Robot Interaction in Simulation","description":"Introduction","source":"@site/docs/module-2-digital-twin/human-robot-interaction.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/human-robot-interaction","permalink":"/giaic-hackathon-speckit-plus/docs/module-2-digital-twin/human-robot-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-digital-twin/human-robot-interaction.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Digital Twin Environments: Gazebo Usage and Best Practices","permalink":"/giaic-hackathon-speckit-plus/docs/module-2-digital-twin/environments"},"next":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","permalink":"/giaic-hackathon-speckit-plus/docs/module-3-ai-brain/"}}');var i=t(4848),a=t(8453);const r={sidebar_position:7},o="Human-Robot Interaction in Simulation",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Types of Human-Robot Interaction in Simulation",id:"types-of-human-robot-interaction-in-simulation",level:2},{value:"1. Teleoperation",id:"1-teleoperation",level:3},{value:"2. Voice Command Simulation",id:"2-voice-command-simulation",level:3},{value:"3. Gesture Recognition Simulation",id:"3-gesture-recognition-simulation",level:3},{value:"Safety in Human-Robot Interaction Simulation",id:"safety-in-human-robot-interaction-simulation",level:2},{value:"Safety Zones and Boundaries",id:"safety-zones-and-boundaries",level:3},{value:"Safety Monitoring Node",id:"safety-monitoring-node",level:3},{value:"User Interface Simulation",id:"user-interface-simulation",level:2},{value:"Graphical User Interface for HRI",id:"graphical-user-interface-for-hri",level:3},{value:"Integration with ROS 2 and Gazebo",id:"integration-with-ros-2-and-gazebo",level:2},{value:"Connecting Simulation to Real Interaction",id:"connecting-simulation-to-real-interaction",level:3},{value:"Connecting Simulation to ROS 2 Systems",id:"connecting-simulation-to-ros-2-systems",level:3},{value:"1. Gazebo ROS Plugins",id:"1-gazebo-ros-plugins",level:4},{value:"2. Topic Mapping",id:"2-topic-mapping",level:4},{value:"3. Coordinate Frame Management",id:"3-coordinate-frame-management",level:4},{value:"4. Launch Configuration",id:"4-launch-configuration",level:4},{value:"5. Simulation vs. Real Robot Configuration",id:"5-simulation-vs-real-robot-configuration",level:4},{value:"Testing HRI Scenarios",id:"testing-hri-scenarios",level:2},{value:"Scenario Testing Framework",id:"scenario-testing-framework",level:3},{value:"Best Practices for HRI Simulation",id:"best-practices-for-hri-simulation",level:2},{value:"1. Realistic Human Modeling",id:"1-realistic-human-modeling",level:3},{value:"2. Gradual Complexity Increase",id:"2-gradual-complexity-increase",level:3},{value:"3. Comprehensive Safety Testing",id:"3-comprehensive-safety-testing",level:3},{value:"4. Multi-Modal Interaction",id:"4-multi-modal-interaction",level:3},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"human-robot-interaction-in-simulation",children:"Human-Robot Interaction in Simulation"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Human-Robot Interaction (HRI) in simulation provides a safe and controlled environment to develop, test, and validate interaction paradigms before deploying them on real robots. Simulation allows for the exploration of various interaction modalities, user interface designs, and safety protocols without the risks associated with physical human-robot contact. This guide covers how to implement and test human-robot interaction scenarios in digital twin environments."}),"\n",(0,i.jsx)(n.h2,{id:"types-of-human-robot-interaction-in-simulation",children:"Types of Human-Robot Interaction in Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"1-teleoperation",children:"1. Teleoperation"}),"\n",(0,i.jsx)(n.p,{children:"Teleoperation allows humans to remotely control robots through various interfaces. Simulation provides a safe environment to test teleoperation interfaces:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example world with teleoperation elements --\x3e\n<sdf version="1.6">\n  <world name="teleop_world">\n    <physics type="ode">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n      <gravity>0 0 -9.8</gravity>\n    </physics>\n\n    \x3c!-- Robot that can be teleoperated --\x3e\n    <model name="teleop_robot">\n      <pose>0 0 0.1 0 0 0</pose>\n      <link name="base_link">\n        <inertial>\n          <mass>10.0</mass>\n          <inertia><ixx>1.0</ixx><iyy>1.0</iyy><izz>1.0</izz></inertia>\n        </inertial>\n        <visual name="visual">\n          <geometry><cylinder><radius>0.3</radius><length>0.2</length></cylinder></geometry>\n          <material><diffuse>0.2 0.6 1.0 1</diffuse></material>\n        </visual>\n        <collision name="collision">\n          <geometry><cylinder><radius>0.3</radius><length>0.2</length></cylinder></geometry>\n        </collision>\n      </link>\n\n      \x3c!-- Differential drive plugin for mobile base --\x3e\n      <plugin name="diff_drive" filename="libgazebo_ros_diff_drive.so">\n        <command_topic>cmd_vel</command_topic>\n        <odometry_topic>odom</odometry_topic>\n        <odometry_frame>odom</odometry_frame>\n        <robot_base_frame>base_link</robot_base_frame>\n        <publish_odom>true</publish_odom>\n        <publish_odom_tf>true</publish_odom_tf>\n        <publish_wheel_tf>false</publish_wheel_tf>\n        <wheel_separation>0.4</wheel_separation>\n        <wheel_diameter>0.15</wheel_diameter>\n        <max_wheel_torque>20.0</max_wheel_torque>\n        <max_wheel_acceleration>1.0</max_wheel_acceleration>\n      </plugin>\n    </model>\n\n    \x3c!-- Objects for interaction --\x3e\n    <model name="interactive_object">\n      <pose>2 0 0.5 0 0 0</pose>\n      <link name="object_link">\n        <inertial>\n          <mass>0.5</mass>\n          <inertia><ixx>0.01</ixx><iyy>0.01</iyy><izz>0.01</izz></inertia>\n        </inertial>\n        <visual name="visual">\n          <geometry><box><size>0.2 0.2 0.2</size></box></geometry>\n          <material><diffuse>1.0 0.5 0.0 1</diffuse></material>\n        </visual>\n        <collision name="collision">\n          <geometry><box><size>0.2 0.2 0.2</size></box></geometry>\n        </collision>\n      </link>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-voice-command-simulation",children:"2. Voice Command Simulation"}),"\n",(0,i.jsx)(n.p,{children:"Simulating voice command systems allows for testing natural language interfaces:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nimport speech_recognition as sr\nimport pyttsx3\nimport threading\nimport time\n\nclass VoiceCommandSimulator(Node):\n    """\n    Simulates voice command processing for human-robot interaction.\n    In real applications, this would interface with actual speech recognition.\n    """\n\n    def __init__(self):\n        super().__init__(\'voice_command_simulator\')\n\n        # Publisher for robot commands\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Publisher for voice feedback\n        self.speech_pub = self.create_publisher(String, \'/robot_speech\', 10)\n\n        # Subscriber for laser scan to detect obstacles\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10)\n\n        # Voice command processing\n        self.last_scan = None\n        self.speech_engine = pyttsx3.init()\n\n        # Start voice command processing in a separate thread\n        self.voice_thread = threading.Thread(target=self.process_voice_commands)\n        self.voice_thread.daemon = True\n        self.voice_thread.start()\n\n    def scan_callback(self, msg):\n        """Update last scan data"""\n        self.last_scan = msg\n\n    def process_voice_commands(self):\n        """Simulate voice command processing"""\n        # In simulation, we\'ll use predefined commands instead of real speech recognition\n        # This simulates what would happen with actual voice input\n\n        commands = [\n            "move forward",\n            "turn left",\n            "stop",\n            "go to the kitchen"\n        ]\n\n        for command in commands:\n            self.get_logger().info(f"Processing command: {command}")\n            self.execute_command(command)\n            time.sleep(3)  # Wait between commands\n\n    def execute_command(self, command):\n        """Execute voice command"""\n        cmd = Twist()\n\n        if "forward" in command:\n            cmd.linear.x = 0.5\n            self.get_logger().info("Moving forward")\n        elif "backward" in command:\n            cmd.linear.x = -0.5\n            self.get_logger().info("Moving backward")\n        elif "left" in command:\n            cmd.angular.z = 0.5\n            self.get_logger().info("Turning left")\n        elif "right" in command:\n            cmd.angular.z = -0.5\n            self.get_logger().info("Turning right")\n        elif "stop" in command:\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n            self.get_logger().info("Stopping")\n        else:\n            self.get_logger().info(f"Unknown command: {command}")\n            return\n\n        # Check for obstacles before moving\n        if self.last_scan and self.check_obstacles():\n            self.get_logger().warn("Obstacle detected, stopping")\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n\n            # Publish warning\n            warning_msg = String()\n            warning_msg.data = "Obstacle detected, cannot move forward"\n            self.speech_pub.publish(warning_msg)\n        else:\n            self.cmd_pub.publish(cmd)\n\n    def check_obstacles(self):\n        """Check if there are obstacles in front of the robot"""\n        if not self.last_scan:\n            return False\n\n        # Check the front 30 degrees for obstacles\n        front_range = len(self.last_scan.ranges) // 2\n        scan_width = len(self.last_scan.ranges) // 12  # ~30 degrees\n\n        for i in range(front_range - scan_width, front_range + scan_width):\n            if i >= 0 and i < len(self.last_scan.ranges):\n                if self.last_scan.ranges[i] < 0.5:  # Obstacle within 50cm\n                    return True\n        return False\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_simulator = VoiceCommandSimulator()\n\n    try:\n        rclpy.spin(voice_simulator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_simulator.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-gesture-recognition-simulation",children:"3. Gesture Recognition Simulation"}),"\n",(0,i.jsx)(n.p,{children:"Simulating gesture recognition for intuitive human-robot interaction:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass GestureRecognitionSimulator(Node):\n    """\n    Simulates gesture recognition for human-robot interaction.\n    In simulation, this processes camera images to detect gestures.\n    """\n\n    def __init__(self):\n        super().__init__(\'gesture_recognition_simulator\')\n\n        # Publisher for robot commands\n        self.cmd_pub = self.create_publisher(Twift, \'/cmd_vel\', 10)\n\n        # Publisher for gesture recognition feedback\n        self.gesture_pub = self.create_publisher(String, \'/gesture_recognition\', 10)\n\n        # Subscriber for camera images\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n\n        self.bridge = CvBridge()\n        self.gesture_history = []\n\n    def image_callback(self, msg):\n        """Process camera image for gesture recognition"""\n        try:\n            # Convert ROS image to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Process the image to detect gestures\n            gesture = self.detect_gesture(cv_image)\n\n            if gesture:\n                self.get_logger().info(f"Detected gesture: {gesture}")\n\n                # Execute corresponding action\n                self.execute_gesture_action(gesture)\n\n                # Publish gesture recognition result\n                gesture_msg = String()\n                gesture_msg.data = gesture\n                self.gesture_pub.publish(gesture_msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {e}")\n\n    def detect_gesture(self, image):\n        """Detect gesture from image (simplified for simulation)"""\n        # In a real implementation, this would use computer vision\n        # or machine learning to detect gestures\n\n        # For simulation, we\'ll use a simplified approach\n        # that detects hand positions in a specific area\n\n        # Define ROI for gesture detection (simplified)\n        height, width = image.shape[:2]\n        roi = image[height//2:, width//4:3*width//4]  # Bottom half, middle third\n\n        # Convert to HSV for better color detection\n        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n\n        # Define range for skin color (simplified)\n        lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n        upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n\n        # Create mask\n        mask = cv2.inRange(hsv, lower_skin, upper_skin)\n\n        # Apply morphological operations to clean up the mask\n        kernel = np.ones((5,5), np.uint8)\n        mask = cv2.dilate(mask, kernel, iterations=1)\n        mask = cv2.GaussianBlur(mask, (3, 3), 0)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n        if contours:\n            # Find the largest contour\n            largest_contour = max(contours, key=cv2.contourArea)\n\n            if cv2.contourArea(largest_contour) > 5000:  # Minimum area threshold\n                # Calculate the center of the contour\n                M = cv2.moments(largest_contour)\n                if M["m00"] != 0:\n                    cx = int(M["m10"] / M["m00"])\n                    cy = int(M["m01"] / M["m00"])\n\n                    # Determine gesture based on position\n                    roi_width = roi.shape[1]\n                    if cx < roi_width * 0.33:\n                        return "left"\n                    elif cx > roi_width * 0.66:\n                        return "right"\n                    else:\n                        return "forward"\n\n        return None\n\n    def execute_gesture_action(self, gesture):\n        """Execute action based on detected gesture"""\n        cmd = Twist()\n\n        if gesture == "forward":\n            cmd.linear.x = 0.3\n            cmd.angular.z = 0.0\n        elif gesture == "left":\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.3\n        elif gesture == "right":\n            cmd.linear.x = 0.0\n            cmd.angular.z = -0.3\n        else:\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n\n        self.cmd_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    gesture_simulator = GestureRecognitionSimulator()\n\n    try:\n        rclpy.spin(gesture_simulator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        gesture_simulator.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"safety-in-human-robot-interaction-simulation",children:"Safety in Human-Robot Interaction Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"safety-zones-and-boundaries",children:"Safety Zones and Boundaries"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example with safety zones --\x3e\n<world name="safe_hri_world">\n  <physics type="ode">\n    <max_step_size>0.001</max_step_size>\n    <real_time_factor>1.0</real_time_factor>\n    <gravity>0 0 -9.8</gravity>\n  </physics>\n\n  \x3c!-- Human model (for simulation purposes) --\x3e\n  <model name="human">\n    <pose>3 0 0.75 0 0 0</pose>\n    <link name="human_link">\n      <visual name="visual">\n        <geometry><cylinder><radius>0.2</radius><length>1.5</length></cylinder></geometry>\n        <material><diffuse>0.8 0.2 0.8 1</diffuse></material>\n      </visual>\n      <collision name="collision">\n        <geometry><cylinder><radius>0.2</radius><length>1.5</length></cylinder></geometry>\n      </collision>\n      <inertial>\n        <mass>70.0</mass>\n        <inertia><ixx>10.0</ixx><iyy>10.0</iyy><izz>5.0</izz></inertia>\n      </inertial>\n    </link>\n  </model>\n\n  \x3c!-- Robot with safety awareness --\x3e\n  <model name="safe_robot">\n    <pose>0 0 0.1 0 0 0</pose>\n    <link name="base_link">\n      <visual name="visual">\n        <geometry><cylinder><radius>0.3</radius><length>0.2</length></cylinder></geometry>\n        <material><diffuse>0.2 0.6 1.0 1</diffuse></material>\n      </visual>\n      <collision name="collision">\n        <geometry><cylinder><radius>0.3</radius><length>0.2</length></cylinder></geometry>\n      </collision>\n    </link>\n\n    \x3c!-- Safety zone (invisible) --\x3e\n    <model name="safety_zone">\n      <static>true</static>\n      <link name="zone_link">\n        <visual name="visual">\n          <geometry><sphere><radius>1.0</radius></sphere></geometry>\n          <material><diffuse>1.0 0.0 0.0 0.2</diffuse></material>\n        </visual>\n        <collision name="collision">\n          <geometry><sphere><radius>1.0</radius></sphere></geometry>\n        </collision>\n      </link>\n      <pose>0 0 0.5 0 0 0</pose>\n    </model>\n  </model>\n</world>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"safety-monitoring-node",children:"Safety Monitoring Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Point, PoseStamped\nfrom sensor_msgs.msg import LaserScan\nfrom std_msgs.msg import Bool, String\nimport math\n\nclass SafetyMonitor(Node):\n    """\n    Monitors safety in human-robot interaction scenarios.\n    """\n\n    def __init__(self):\n        super().__init__(\'safety_monitor\')\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10)\n\n        self.human_pose_sub = self.create_subscription(\n            PoseStamped, \'/human_pose\', self.human_pose_callback, 10)\n\n        # Publishers\n        self.safety_pub = self.create_publisher(Bool, \'/safety_status\', 10)\n        self.emergency_stop_pub = self.create_publisher(Bool, \'/emergency_stop\', 10)\n        self.warning_pub = self.create_publisher(String, \'/safety_warning\', 10)\n\n        # Parameters\n        self.safety_distance = 1.0  # meters\n        self.human_position = None\n        self.robot_position = None\n\n        # Timer for safety checks\n        self.timer = self.create_timer(0.1, self.safety_check)\n\n    def scan_callback(self, msg):\n        """Process laser scan to detect humans or obstacles"""\n        # In simulation, we might get human detection from other nodes\n        # This is a simplified version that checks for close obstacles\n        min_distance = min([r for r in msg.ranges if not math.isnan(r) and r > 0], default=float(\'inf\'))\n\n        if min_distance < self.safety_distance:\n            self.trigger_safety_alert(f"Obstacle detected at {min_distance:.2f}m, within safety distance of {self.safety_distance}m")\n\n    def human_pose_callback(self, msg):\n        """Update human position"""\n        self.human_position = msg.pose.position\n\n    def safety_check(self):\n        """Check safety conditions"""\n        safety_status = Bool()\n        safety_status.data = True  # Assume safe by default\n\n        if self.human_position:\n            # Calculate distance to human\n            distance = math.sqrt(\n                self.human_position.x**2 +\n                self.human_position.y**2 +\n                self.human_position.z**2\n            )\n\n            if distance < self.safety_distance:\n                safety_status.data = False\n                self.trigger_safety_alert(f"Human detected at {distance:.2f}m, within safety distance of {self.safety_distance}m")\n\n        self.safety_pub.publish(safety_status)\n\n    def trigger_safety_alert(self, message):\n        """Trigger safety alert and emergency stop if needed"""\n        self.get_logger().warn(message)\n\n        # Publish warning\n        warning_msg = String()\n        warning_msg.data = message\n        self.warning_pub.publish(warning_msg)\n\n        # Publish emergency stop\n        emergency_msg = Bool()\n        emergency_msg.data = True\n        self.emergency_stop_pub.publish(emergency_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    safety_monitor = SafetyMonitor()\n\n    try:\n        rclpy.spin(safety_monitor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        safety_monitor.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"user-interface-simulation",children:"User Interface Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"graphical-user-interface-for-hri",children:"Graphical User Interface for HRI"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import tkinter as tk\nfrom tkinter import ttk\nimport rclpy\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport threading\n\nclass HRIGUI:\n    """\n    Graphical user interface for human-robot interaction simulation.\n    """\n\n    def __init__(self, node):\n        self.node = node\n        self.root = tk.Tk()\n        self.root.title("Human-Robot Interaction Simulator")\n        self.root.geometry("400x300")\n\n        # Publisher for robot commands\n        self.cmd_pub = node.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Create GUI elements\n        self.create_widgets()\n\n        # Start GUI in separate thread\n        self.gui_thread = threading.Thread(target=self.run_gui)\n        self.gui_thread.daemon = True\n        self.gui_thread.start()\n\n    def create_widgets(self):\n        """Create GUI widgets"""\n        # Main frame\n        main_frame = ttk.Frame(self.root, padding="10")\n        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n\n        # Title\n        title_label = ttk.Label(main_frame, text="Robot Control Interface", font=("Arial", 14, "bold"))\n        title_label.grid(row=0, column=0, columnspan=3, pady=10)\n\n        # Direction buttons\n        self.up_btn = ttk.Button(main_frame, text="\u2191 Forward", command=self.move_forward)\n        self.up_btn.grid(row=1, column=1, pady=5)\n\n        self.left_btn = ttk.Button(main_frame, text="\u2190 Left", command=self.move_left)\n        self.left_btn.grid(row=2, column=0, padx=5, pady=5)\n\n        self.stop_btn = ttk.Button(main_frame, text="\u25cf Stop", command=self.stop_robot)\n        self.stop_btn.grid(row=2, column=1, pady=5)\n\n        self.right_btn = ttk.Button(main_frame, text="\u2192 Right", command=self.move_right)\n        self.right_btn.grid(row=2, column=2, padx=5, pady=5)\n\n        self.down_btn = ttk.Button(main_frame, text="\u2193 Backward", command=self.move_backward)\n        self.down_btn.grid(row=3, column=1, pady=5)\n\n        # Speed control\n        speed_frame = ttk.Frame(main_frame)\n        speed_frame.grid(row=4, column=0, columnspan=3, pady=10)\n\n        ttk.Label(speed_frame, text="Speed:").pack(side=tk.LEFT)\n        self.speed_var = tk.DoubleVar(value=0.5)\n        speed_scale = ttk.Scale(speed_frame, from_=0.1, to=1.0, variable=self.speed_var, orient=tk.HORIZONTAL)\n        speed_scale.pack(side=tk.LEFT, padx=5, fill=tk.X, expand=True)\n\n        # Voice command simulation\n        voice_frame = ttk.Frame(main_frame)\n        voice_frame.grid(row=5, column=0, columnspan=3, pady=10)\n\n        ttk.Label(voice_frame, text="Voice Command:").pack(anchor=tk.W)\n\n        self.voice_entry = ttk.Entry(voice_frame, width=30)\n        self.voice_entry.pack(side=tk.LEFT, padx=5)\n        self.voice_entry.bind(\'<Return>\', self.process_voice_command)\n\n        voice_btn = ttk.Button(voice_frame, text="Send", command=self.process_voice_command)\n        voice_btn.pack(side=tk.LEFT)\n\n    def move_forward(self):\n        """Move robot forward"""\n        cmd = Twist()\n        cmd.linear.x = self.speed_var.get()\n        self.cmd_pub.publish(cmd)\n\n    def move_backward(self):\n        """Move robot backward"""\n        cmd = Twist()\n        cmd.linear.x = -self.speed_var.get()\n        self.cmd_pub.publish(cmd)\n\n    def move_left(self):\n        """Turn robot left"""\n        cmd = Twist()\n        cmd.angular.z = self.speed_var.get()\n        self.cmd_pub.publish(cmd)\n\n    def move_right(self):\n        """Turn robot right"""\n        cmd = Twist()\n        cmd.angular.z = -self.speed_var.get()\n        self.cmd_pub.publish(cmd)\n\n    def stop_robot(self):\n        """Stop robot"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\n    def process_voice_command(self, event=None):\n        """Process voice command from entry"""\n        command = self.voice_entry.get()\n        self.node.get_logger().info(f"Processing voice command: {command}")\n\n        # Clear entry\n        self.voice_entry.delete(0, tk.END)\n\n        # Execute command\n        if "forward" in command.lower():\n            self.move_forward()\n        elif "backward" in command.lower():\n            self.move_backward()\n        elif "left" in command.lower():\n            self.move_left()\n        elif "right" in command.lower():\n            self.move_right()\n        elif "stop" in command.lower():\n            self.stop_robot()\n\n    def run_gui(self):\n        """Run GUI main loop"""\n        self.root.mainloop()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = rclpy.create_node(\'hri_gui_node\')\n\n    # Create GUI\n    gui = HRIGUI(node)\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-ros-2-and-gazebo",children:"Integration with ROS 2 and Gazebo"}),"\n",(0,i.jsx)(n.h3,{id:"connecting-simulation-to-real-interaction",children:"Connecting Simulation to Real Interaction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String, Bool\nfrom visualization_msgs.msg import Marker\nfrom tf2_ros import TransformListener\nimport math\n\nclass HRIIntegrationNode(Node):\n    """\n    Integrates various HRI components in simulation.\n    """\n\n    def __init__(self):\n        super().__init__(\'hri_integration_node\')\n\n        # Publishers\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.marker_pub = self.create_publisher(Marker, \'/hri_markers\', 10)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(LaserScan, \'/scan\', self.scan_callback, 10)\n        self.image_sub = self.create_subscription(Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, \'/imu\', self.imu_callback, 10)\n        self.voice_sub = self.create_subscription(String, \'/voice_commands\', self.voice_callback, 10)\n        self.safety_sub = self.create_subscription(Bool, \'/safety_status\', self.safety_callback, 10)\n\n        # TF listener for transforms\n        self.tf_buffer = rclpy.buffer.Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # State variables\n        self.safety_status = True\n        self.last_voice_command = ""\n\n        # Timer for periodic updates\n        self.timer = self.create_timer(0.1, self.update)\n\n    def scan_callback(self, msg):\n        """Process laser scan data"""\n        # Process scan data for navigation and safety\n        pass\n\n    def image_callback(self, msg):\n        """Process image data for gesture recognition"""\n        # Process image data for gesture recognition\n        pass\n\n    def imu_callback(self, msg):\n        """Process IMU data for robot state"""\n        # Process IMU data for orientation and stability\n        pass\n\n    def voice_callback(self, msg):\n        """Process voice commands"""\n        self.last_voice_command = msg.data\n        self.get_logger().info(f"Received voice command: {msg.data}")\n\n    def safety_callback(self, msg):\n        """Update safety status"""\n        self.safety_status = msg.data\n        if not self.safety_status:\n            self.get_logger().warn("Safety system indicates unsafe conditions")\n\n    def update(self):\n        """Periodic update function"""\n        # Update visualization markers\n        self.publish_markers()\n\n    def publish_markers(self):\n        """Publish visualization markers for HRI"""\n        # Publish markers for safety zones, interaction areas, etc.\n        marker = Marker()\n        marker.header.frame_id = "map"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "hri"\n        marker.id = 0\n        marker.type = Marker.SPHERE\n        marker.action = Marker.ADD\n\n        # Set position (example: safety zone)\n        marker.pose.position.x = 0.0\n        marker.pose.position.y = 0.0\n        marker.pose.position.z = 0.0\n        marker.pose.orientation.w = 1.0\n\n        # Set scale\n        marker.scale.x = 2.0  # Safety radius\n        marker.scale.y = 2.0\n        marker.scale.z = 0.1  # Flat marker\n\n        # Set color (red for safety zone)\n        marker.color.r = 1.0\n        marker.color.g = 0.0\n        marker.color.b = 0.0\n        marker.color.a = 0.3  # Semi-transparent\n\n        self.marker_pub.publish(marker)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    hri_node = HRIIntegrationNode()\n\n    try:\n        rclpy.spin(hri_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        hri_node.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"connecting-simulation-to-ros-2-systems",children:"Connecting Simulation to ROS 2 Systems"}),"\n",(0,i.jsx)(n.p,{children:"To connect Gazebo simulation to ROS 2 systems, several key components must be properly configured:"}),"\n",(0,i.jsx)(n.h4,{id:"1-gazebo-ros-plugins",children:"1. Gazebo ROS Plugins"}),"\n",(0,i.jsx)(n.p,{children:"Gazebo uses plugins to interface with ROS 2. Common plugins include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"libgazebo_ros_diff_drive.so"}),": For differential drive robots"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"libgazebo_ros_camera.so"}),": For camera sensors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"libgazebo_ros_laser.so"}),": For LiDAR sensors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"libgazebo_ros_imu.so"}),": For IMU sensors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"libgazebo_ros_p3d.so"}),": For pose/position tracking"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example configuration in SDF:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<plugin name="diff_drive" filename="libgazebo_ros_diff_drive.so">\n  <command_topic>cmd_vel</command_topic>\n  <odometry_topic>odom</odometry_topic>\n  <odometry_frame>odom</odometry_frame>\n  <robot_base_frame>base_link</robot_base_frame>\n  <publish_odom>true</publish_odom>\n  <publish_odom_tf>true</publish_odom_tf>\n  <publish_wheel_tf>false</publish_wheel_tf>\n  <wheel_separation>0.4</wheel_separation>\n  <wheel_diameter>0.15</wheel_diameter>\n</plugin>\n'})}),"\n",(0,i.jsx)(n.h4,{id:"2-topic-mapping",children:"2. Topic Mapping"}),"\n",(0,i.jsx)(n.p,{children:"Ensure proper topic mapping between simulation and ROS 2 nodes:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# In your ROS 2 node\nclass SimulationController(Node):\n    def __init__(self):\n        super().__init__('simulation_controller')\n\n        # Publishers - match topics defined in Gazebo plugins\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.joint_pub = self.create_publisher(JointState, '/joint_states', 10)\n\n        # Subscribers - match topics published by Gazebo\n        self.laser_sub = self.create_subscription(LaserScan, '/scan', self.scan_callback, 10)\n        self.camera_sub = self.create_subscription(Image, '/camera/image_raw', self.image_callback, 10)\n        self.odom_sub = self.create_subscription(Odometry, '/odom', self.odom_callback, 10)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"3-coordinate-frame-management",children:"3. Coordinate Frame Management"}),"\n",(0,i.jsx)(n.p,{children:"Use TF (Transform) to manage coordinate frames between simulation and real world:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\n\nclass FramePublisher(Node):\n    def __init__(self):\n        super().__init__('frame_publisher')\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Publish transforms to connect simulation frames\n        self.timer = self.create_timer(0.05, self.broadcast_transforms)\n\n    def broadcast_transforms(self):\n        \"\"\"Broadcast coordinate transforms\"\"\"\n        t = TransformStamped()\n\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'odom'\n        t.child_frame_id = 'base_link'\n\n        # Get robot pose from simulation or odometry\n        t.transform.translation.x = 0.0\n        t.transform.translation.y = 0.0\n        t.transform.translation.z = 0.0\n        t.transform.rotation.x = 0.0\n        t.transform.rotation.y = 0.0\n        t.transform.rotation.z = 0.0\n        t.transform.rotation.w = 1.0\n\n        self.tf_broadcaster.sendTransform(t)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"4-launch-configuration",children:"4. Launch Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Create launch files to start both Gazebo and ROS 2 nodes together:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution, LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # World file argument\n    world_arg = DeclareLaunchArgument(\n        'world',\n        default_value=PathJoinSubstitution([\n            FindPackageShare('my_robot_gazebo'),\n            'worlds',\n            'hri_world.sdf'\n        ]),\n        description='SDF world file'\n    )\n\n    # Launch Gazebo\n    gazebo_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('gazebo_ros'),\n                'launch',\n                'gazebo.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'world': LaunchConfiguration('world'),\n            'verbose': 'true'\n        }.items()\n    )\n\n    # Launch robot controller node\n    controller_node = Node(\n        package='my_robot_control',\n        executable='controller_node',\n        name='robot_controller',\n        output='screen'\n    )\n\n    # Launch HRI interface node\n    hri_node = Node(\n        package='my_robot_hri',\n        executable='hri_interface',\n        name='hri_interface',\n        output='screen'\n    )\n\n    return LaunchDescription([\n        world_arg,\n        gazebo_launch,\n        controller_node,\n        hri_node\n    ])\n"})}),"\n",(0,i.jsx)(n.h4,{id:"5-simulation-vs-real-robot-configuration",children:"5. Simulation vs. Real Robot Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Use parameter files to switch between simulation and real robot configurations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# simulation_params.yaml\nrobot_controller:\n  ros__parameters:\n    use_sim_time: true\n    cmd_vel_topic: "/cmd_vel"\n    odom_topic: "/odom"\n    laser_topic: "/scan"\n    camera_topic: "/camera/image_raw"\n\n# real_robot_params.yaml\nrobot_controller:\n  ros__parameters:\n    use_sim_time: false\n    cmd_vel_topic: "/hardware/cmd_vel"\n    odom_topic: "/hardware/odom"\n    laser_topic: "/hardware/scan"\n    camera_topic: "/hardware/camera/image_raw"\n'})}),"\n",(0,i.jsx)(n.p,{children:"This approach allows the same ROS 2 nodes to work in both simulation and real environments with minimal changes."}),"\n",(0,i.jsx)(n.h2,{id:"testing-hri-scenarios",children:"Testing HRI Scenarios"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-testing-framework",children:"Scenario Testing Framework"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nimport time\n\nclass HRITestFramework(Node):\n    """\n    Framework for testing HRI scenarios in simulation.\n    """\n\n    def __init__(self):\n        super().__init__(\'hri_test_framework\')\n\n        # Publishers and subscribers\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.voice_pub = self.create_publisher(String, \'/voice_commands\', 10)\n        self.test_result_pub = self.create_publisher(String, \'/test_results\', 10)\n\n        self.scan_sub = self.create_subscription(LaserScan, \'/scan\', self.scan_callback, 10)\n        self.safety_sub = self.create_subscription(Bool, \'/safety_status\', self.safety_callback, 10)\n\n        # Test state\n        self.current_test = 0\n        self.tests = [\n            self.test_voice_navigation,\n            self.test_obstacle_avoidance,\n            self.test_safety_response,\n            self.test_gesture_recognition\n        ]\n\n        # Start testing\n        self.timer = self.create_timer(1.0, self.run_tests)\n        self.test_start_time = time.time()\n\n    def scan_callback(self, msg):\n        """Process scan data for tests"""\n        self.last_scan = msg\n\n    def safety_callback(self, msg):\n        """Process safety status for tests"""\n        self.safety_status = msg.data\n\n    def run_tests(self):\n        """Run HRI tests sequentially"""\n        if self.current_test < len(self.tests):\n            test_func = self.tests[self.current_test]\n            self.get_logger().info(f"Running test {self.current_test + 1}: {test_func.__name__}")\n\n            # Run the test\n            result = test_func()\n\n            # Publish result\n            result_msg = String()\n            result_msg.data = f"Test {self.current_test + 1} ({test_func.__name__}): {result}"\n            self.test_result_pub.publish(result_msg)\n\n            self.current_test += 1\n        else:\n            # All tests completed\n            self.get_logger().info("All HRI tests completed")\n            self.timer.cancel()\n\n    def test_voice_navigation(self):\n        """Test voice command navigation"""\n        try:\n            # Send voice command\n            cmd = String()\n            cmd.data = "move forward 2 meters"\n            self.voice_pub.publish(cmd)\n\n            # Wait for robot to move\n            time.sleep(3)\n\n            # Check if robot moved appropriately\n            # (In simulation, we\'d check position, in this example we just return success)\n            return "PASSED"\n        except Exception as e:\n            return f"FAILED: {str(e)}"\n\n    def test_obstacle_avoidance(self):\n        """Test obstacle avoidance during interaction"""\n        try:\n            # Send movement command\n            cmd = Twist()\n            cmd.linear.x = 0.5\n            self.cmd_pub.publish(cmd)\n\n            # Wait and check if safety system activates\n            time.sleep(2)\n\n            # Stop robot\n            stop_cmd = Twist()\n            stop_cmd.linear.x = 0.0\n            self.cmd_pub.publish(stop_cmd)\n\n            return "PASSED"\n        except Exception as e:\n            return f"FAILED: {str(e)}"\n\n    def test_safety_response(self):\n        """Test safety system response"""\n        try:\n            # This would test the safety system in a real scenario\n            # For simulation, we check if safety status is being monitored\n            if hasattr(self, \'safety_status\'):\n                return "PASSED"\n            else:\n                return "FAILED: Safety status not available"\n        except Exception as e:\n            return f"FAILED: {str(e)}"\n\n    def test_gesture_recognition(self):\n        """Test gesture recognition (simulated)"""\n        try:\n            # In simulation, this would trigger gesture recognition\n            # For now, we just return PASSED\n            return "PASSED"\n        except Exception as e:\n            return f"FAILED: {str(e)}"\n\ndef main(args=None):\n    rclpy.init(args=args)\n    test_framework = HRITestFramework()\n\n    try:\n        rclpy.spin(test_framework)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        test_framework.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-for-hri-simulation",children:"Best Practices for HRI Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"1-realistic-human-modeling",children:"1. Realistic Human Modeling"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Model human behavior patterns realistically"}),"\n",(0,i.jsx)(n.li,{children:"Include reaction times and decision-making delays"}),"\n",(0,i.jsx)(n.li,{children:"Consider human comfort zones and safety preferences"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-gradual-complexity-increase",children:"2. Gradual Complexity Increase"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Start with simple interaction scenarios"}),"\n",(0,i.jsx)(n.li,{children:"Gradually add complexity (multiple humans, dynamic environments)"}),"\n",(0,i.jsx)(n.li,{children:"Test each level before proceeding"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-comprehensive-safety-testing",children:"3. Comprehensive Safety Testing"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Test all safety scenarios thoroughly"}),"\n",(0,i.jsx)(n.li,{children:"Include edge cases and unexpected situations"}),"\n",(0,i.jsx)(n.li,{children:"Validate safety system responses"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-multi-modal-interaction",children:"4. Multi-Modal Interaction"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Combine different interaction modalities (voice, gesture, touch)"}),"\n",(0,i.jsx)(n.li,{children:"Test cross-modal consistency"}),"\n",(0,i.jsx)(n.li,{children:"Ensure fallback mechanisms"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Human-Robot Interaction simulation provides a crucial testing ground for developing safe and effective interaction paradigms. By simulating various interaction modalities, safety systems, and user interfaces, we can validate HRI concepts before deploying them on physical robots. The examples provided demonstrate how to implement teleoperation, voice commands, gesture recognition, safety monitoring, and comprehensive testing frameworks in simulation environments. These approaches enable the development of intuitive, safe, and effective human-robot interaction systems for Physical AI applications."})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);